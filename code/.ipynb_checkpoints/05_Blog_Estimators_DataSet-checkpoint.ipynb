{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will just run the data first and then run the whole thing.\n",
    "\n",
    "### Remember to restart the Kernel before running again.\n",
    "\n",
    "#### The warning about QueueRunner is a TensorFlow bug. Don't worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.3.0\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/jmugan/temp/tf_dataset_and_estimator_apis', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /Users/jmugan/temp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:loss = 52.255, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 30 into /Users/jmugan/temp/tf_dataset_and_estimator_apis/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 12.1572.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-11-16:17:08\n",
      "INFO:tensorflow:Restoring parameters from /Users/jmugan/temp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-11-16:17:08\n",
      "INFO:tensorflow:Saving dict for global step 30: accuracy = 0.7, average_loss = 0.448881, global_step = 30, loss = 13.4664\n",
      "Evaluation results\n",
      "   accuracy, was: 0.699999988079071\n",
      "   average_loss, was: 0.44888123869895935\n",
      "   loss, was: 13.466437339782715\n",
      "   global_step, was: 30\n",
      "Predictions on test file\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jmugan/temp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "Predictions on memory\n",
      "WARNING:tensorflow:Input graph does not contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Restoring parameters from /Users/jmugan/temp/tf_dataset_and_estimator_apis/model.ckpt-30\n",
      "I think: [5.9, 3.0, 4.2, 1.5], is Iris Virginica\n",
      "I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "# This is the complete code for the following blogpost:\n",
    "# https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html\n",
    "#   (https://goo.gl/Ujm2Ep)\n",
    "\n",
    "# This code copied and modified from https://github.com/mhyttsten/Misc/blob/master/Blog_Estimators_DataSet.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "if sys.version_info < (3, 0, 0):\n",
    "    from urllib import urlopen\n",
    "else:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))\n",
    "assert \"1.3\" <= tf_version, \"TensorFlow r1.3 or later is needed\"\n",
    "\n",
    "# Windows users: You only need to change PATH, rest is platform independent\n",
    "PATH = \"/Users/jmugan/temp/tf_dataset_and_estimator_apis\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"dataset\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "\n",
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# The CSV features in our training & test data\n",
    "feature_names = [\n",
    "    'SepalLength',\n",
    "    'SepalWidth',\n",
    "    'PetalLength',\n",
    "    'PetalWidth']\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "\n",
    "\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1:]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.contrib.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "next_batch = my_input_fn(FILE_TRAIN, True)  # Will return 32 random elements\n",
    "\n",
    "\n",
    "#*****************************************************************************\n",
    "#********************* Try out the data. Change 'if' to run whole thing ******\n",
    "#*****************************************************************************\n",
    "# Now let's try it out, retrieving and printing one batch of data.\n",
    "# Although this code looks strange, you don't need to understand\n",
    "# the details.\n",
    "if 1:\n",
    "    with tf.Session() as sess:\n",
    "        first_batch = sess.run(next_batch)\n",
    "    print(first_batch)\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "\n",
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
    "\n",
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,  # The input features to our model\n",
    "    hidden_units=[10, 10],  # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=PATH)  # Path to where checkpoints etc are stored\n",
    "\n",
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))\n",
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "\n",
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat only once.\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
    "    print(prediction[\"class_ids\"][0])\n",
    "\n",
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
